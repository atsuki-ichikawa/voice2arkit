# MediaPipe ARKitâ€‘BlendShape Extractor

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ **MediaPipe Face Landmarker v2** ã‚’ç”¨ã„ã¦ã€éŒ²ç”»æ¸ˆã¿ãƒ“ãƒ‡ã‚ªã‹ã‚‰ ARKit äº’æ›ã® 52 ãƒ–ãƒ¬ãƒ³ãƒ‰ã‚·ã‚§ã‚¤ãƒ—ä¿‚æ•°ã‚’ CSV/JSON ã¸æ›¸ãå‡ºã™ Python ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æœ€å°æ§‹æˆã§ã™ã€‚

---

## 1. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

```
mediapipe_blendshape_project/
â”œâ”€â”€ main.py                     # ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
â”œâ”€â”€ face_landmarker.py          # MediaPipe ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ smoothing.py            # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ä¿‚æ•°ã®æ™‚ç³»åˆ—å¹³æ»‘åŒ–
â”œâ”€â”€ models/
â”‚   â””â”€â”€ face_landmarker_with_blendshapes.task  # å…¬å¼ãƒ¢ãƒ‡ãƒ«ã‚’é…ç½®
â”œâ”€â”€ requirements.txt            # ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
â””â”€â”€ README.md                   # ä½¿ã„æ–¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
```

---

## 2. `main.py`ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆï¼‰

```python
"""
ä½¿ã„æ–¹:
    python main.py --video input.mp4 --output output.csv

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
    --format json     # JSON å‡ºåŠ›ã‚‚å¯èƒ½
    --smooth yes      # å¹³æ»‘åŒ–ãƒ•ã‚£ãƒ«ã‚¿ã‚’æœ‰åŠ¹åŒ–
"""
from face_landmarker import FaceLandmarkerRunner
import cv2, argparse, csv, json
from pathlib import Path


def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--video", required=True)
    p.add_argument("--output", default="blendshapes.csv")
    p.add_argument("--format", choices=["csv", "json"], default="csv")
    p.add_argument("--smooth", choices=["yes", "no"], default="no")
    return p.parse_args()


def main():
    args = parse_args()
    runner = FaceLandmarkerRunner("models/face_landmarker_with_blendshapes.task")

    cap = cv2.VideoCapture(args.video)
    fps = cap.get(cv2.CAP_PROP_FPS) or 30
    frame_idx = 0

    # å‡ºåŠ›å…ˆæº–å‚™
    if args.format == "csv":
        f = open(args.output, "w", newline="")
        writer = csv.writer(f)
        writer.writerow(["frame"] + runner.blendshape_names)
    else:
        all_frames = []

    while True:
        ok, frame = cap.read()
        if not ok:
            break
        ts_ms = int(1000 * frame_idx / fps)
        coeffs = runner.process_frame(frame, ts_ms)
        if args.format == "csv":
            writer.writerow([frame_idx] + coeffs)
        else:
            all_frames.append({"frame": frame_idx, **dict(zip(runner.blendshape_names, coeffs))})
        frame_idx += 1

    # å¾Œå‡¦ç†
    if args.format == "json":
        Path(args.output).write_text(json.dumps(all_frames, ensure_ascii=False, indent=2))
    cap.release()
    print(f"Done. Wrote {frame_idx} frames â†’ {args.output}")

if __name__ == "__main__":
    main()
```

---

## 3. `face_landmarker.py`ï¼ˆMediaPipe ãƒ©ãƒƒãƒ‘ãƒ¼ï¼‰

```python
import mediapipe as mp
import numpy as np

BaseOptions = mp.tasks.BaseOptions
FaceLandmarker = mp.tasks.vision.FaceLandmarker
FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions
VisionRunningMode = mp.tasks.vision.RunningMode

class FaceLandmarkerRunner:
    """MediaPipe Face Landmarker v2 ã§ãƒ–ãƒ¬ãƒ³ãƒ‰ã‚·ã‚§ã‚¤ãƒ—ä¿‚æ•°ã‚’å–å¾—ã™ã‚‹ç°¡æ˜“ã‚¯ãƒ©ã‚¹"""

    def __init__(self, model_path: str, num_faces: int = 1):
        self._options = FaceLandmarkerOptions(
            base_options=BaseOptions(model_asset_path=model_path),
            output_face_blendshapes=True,
            output_facial_transformation_matrixes=False,
            running_mode=VisionRunningMode.VIDEO,
            num_faces=num_faces,
        )
        self._landmarker = FaceLandmarker.create_from_options(self._options)
        # 52 ã®ä¿‚æ•°åã‚’å–å¾—ï¼ˆMediaPipe ãŒå®šç¾©é †ã‚’ä¿æŒï¼‰
        self.blendshape_names = [
            bs.category_name for bs in
            self._landmarker.detect_for_video  # å‹ãƒ’ãƒ³ãƒˆç”¨ãƒ€ãƒŸãƒ¼å‚ç…§
        ][0:0]  # å®Ÿéš›ã«ã¯æœ€åˆã® frame å‡¦ç†å¾Œã«åŸ‹ã‚è¾¼ã¿

    def _lazy_init_names(self, blendshapes):
        if not self.blendshape_names:
            self.blendshape_names = [b.category_name for b in blendshapes]

    def process_frame(self, frame_bgr, timestamp_ms):
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_bgr)
        result = self._landmarker.detect_for_video(mp_image, timestamp_ms)
        if result.face_blendshapes:
            self._lazy_init_names(result.face_blendshapes[0])
            return [b.score for b in result.face_blendshapes[0]]
        return [0.0] * 52  # æ¤œå‡ºãªã—ã®å ´åˆ
```

---

## 4. `utils/smoothing.py`ï¼ˆä»»æ„ï¼‰

```python
def ema_filter(sequence, alpha=0.5):
    """æŒ‡æ•°ç§»å‹•å¹³å‡ã§ãƒã‚¤ã‚ºä½æ¸›"""
    smoothed = []
    for x in sequence:
        if not smoothed:
            smoothed.append(x)
        else:
            smoothed.append(alpha * x + (1 - alpha) * smoothed[-1])
    return smoothed
```

---

## 5. `requirements.txt`

```
mediapipe>=0.12.0
opencv-python>=4.9.0
numpy>=1.25
```

---

## 6. `README.md`ï¼ˆæŠœç²‹ï¼‰

````markdown
# MediaPipe ARKitâ€‘BlendShape Extractor

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
````

## ä½¿ã„æ–¹

```bash
python main.py --video sample.mp4 --output result.csv
```

* `--format json` ã§ JSON å‡ºåŠ›
* `--smooth yes` ã§ç°¡æ˜“ EMA å¹³æ»‘åŒ–ã‚’å®Ÿè¡Œ

## ãƒ©ã‚¤ãƒ–ã‚«ãƒ¡ãƒ©ã§ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿè¡Œï¼ˆå‚è€ƒï¼‰

`VisionRunningMode.LIVE_STREAM` ã¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å‡¦ç†ã§ãã¾ã™ã€‚




### ã“ã‚Œã§æœ€ä½é™ã®é¡”ãƒ–ãƒ¬ãƒ³ãƒ‰ã‚·ã‚§ã‚¤ãƒ—æŠ½å‡ºã‚¢ãƒ—ãƒªãŒå®Œæˆã—ã¾ã™ã€‚
1. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ `models/` ã«é…ç½®ã€‚  
2. `pip install -r requirements.txt` ã§ä¾å­˜ã‚’å°å…¥ã€‚  
3. `python main.py --video your_video.mp4` ã‚’å®Ÿè¡Œã€‚  
4. å‡ºåŠ›ã•ã‚ŒãŸ CSV/JSON ã‚’ 3D ã‚¢ãƒã‚¿ãƒ¼ã® ARKit ãƒ–ãƒ¬ãƒ³ãƒ‰ã‚·ã‚§ã‚¤ãƒ—ã«é©ç”¨ã€‚

> **TODO**: Unity/Three.js å‘ã‘ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€UI ä»˜ããƒ“ãƒ¥ãƒ¼ãƒ¯ãªã©æ‹¡å¼µã‚‚å®¹æ˜“ã§ã™ã€‚

---

# ğŸ¯ çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆmake_dataset.pyï¼‰

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸­æ ¸æ©Ÿèƒ½ã§ã‚ã‚‹çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ `make_dataset.py` ã¯ã€éŸ³å£°Ã—è¡¨æƒ…ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªå‹•æ§‹ç¯‰ã™ã‚‹é«˜åº¦ãªãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

## ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¦‚è¦

### ğŸ”§ å‡¦ç†ãƒ•ãƒ­ãƒ¼

```mermaid
graph LR
    A[å…¥åŠ›å‹•ç”»<br/>input.mp4] --> B[A. VADåˆ†å‰²<br/>webrtcvad]
    B --> C[B. BlendShapeæŠ½å‡º<br/>MediaPipe v2]
    B --> D[C. éŸ³å£°æŠ½å‡º<br/>ffmpeg]
    D --> E[D. éŸ³å£°ç‰¹å¾´é‡<br/>librosa/pyworld]
    C --> F[E. å“è³ªç®¡ç†<br/>ç•°å¸¸å€¤æ¤œå‡ºãƒ»è£œé–“]
    E --> F
    F --> G[F. ãƒ‡ãƒ¼ã‚¿ä¿å­˜<br/>CSV/NPZ/JSON]
    G --> H[çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ<br/>HTML/JSON]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#fce4ec
    style F fill:#f1f8e9
    style G fill:#e0f2f1
    style H fill:#fff8e1
```

### ğŸ“Š å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿æ§‹é€ 

```mermaid
graph TD
    A[dataset/] --> B[clips/]
    A --> C[data/]
    A --> D[dataset_report.json]
    A --> E[dataset_report.html]
    
    B --> F[clip_0000.mp4<br/>clip_0001.mp4<br/>...]
    
    C --> G[clip_0000_blendshapes.csv<br/>52æ¬¡å…ƒä¿‚æ•°]
    C --> H[clip_0000_audio.wav<br/>16kHz mono]
    C --> I[clip_0000_audio_features.npz<br/>Mel/F0/MFCCç­‰]
    C --> J[clip_0000_quality_mask.npy<br/>å“è³ªãƒ•ãƒ©ã‚°]
    C --> K[clip_0000_metadata.json<br/>çµ±åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿]
    
    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#fff3e0
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬å®Ÿè¡Œ
```bash
python make_dataset.py input.mp4 -o dataset
```

### è©³ç´°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
```bash
python make_dataset.py input.mp4 -o dataset \
  --min_speech 0.4    # æœ€å°ç™ºè©±æ™‚é–“ [ç§’]
  --max_gap 0.6       # çµ±åˆã‚®ãƒ£ãƒƒãƒ—é–¾å€¤ [ç§’]
  --min_clip 20       # æœ€å°ã‚¯ãƒªãƒƒãƒ—é•· [ç§’]
  --max_clip 180      # æœ€å¤§ã‚¯ãƒªãƒƒãƒ—é•· [ç§’]
  --vad_mode 2        # VADæ„Ÿåº¦ (0-3)
  --smooth            # EMAã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°é©ç”¨
```

---

## ğŸ” å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè©³è§£

### A. VADãƒ™ãƒ¼ã‚¹å‹•ç”»åˆ†å‰² (`step_a_split_video`)

**ç›®çš„**: é•·å°ºå‹•ç”»ã‚’éŸ³å£°æ´»å‹•ã«åŸºã¥ã„ã¦æ„å‘³ã®ã‚ã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²

```mermaid
graph TD
    A[å…¥åŠ›å‹•ç”»] --> B[ffmpeg WAVæŠ½å‡º<br/>16kHz mono]
    B --> C[webrtcvad<br/>30msãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æ]
    C --> D[éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ¤œå‡º]
    D --> E[çŸ­ã„ç„¡éŸ³çµ±åˆ<br/>gap < 0.6s]
    E --> F[é•·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²<br/>length > 180s]
    F --> G[ffmpeg -c copy<br/>é«˜é€Ÿåˆ†å‰²]
    G --> H[ã‚¯ãƒªãƒƒãƒ—ç”Ÿæˆ<br/>clip_0000.mp4...]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#fce4ec
    style F fill:#f1f8e9
    style G fill:#e0f2f1
    style H fill:#fff8e1
```

**æŠ€è¡“è©³ç´°**:
- **webrtcvad**: Googleè£½ã®é«˜æ€§èƒ½Voice Activity Detection
- **ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æ**: 30msãƒ•ãƒ¬ãƒ¼ãƒ ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç™ºè©±åˆ¤å®š
- **ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµ±åˆ**: çŸ­ã„ç„¡éŸ³é–“éš”ï¼ˆ<0.6ç§’ï¼‰ã¯çµ±åˆ
- **é•·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²**: 3åˆ†ã‚’è¶…ãˆã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¯ç­‰é–“éš”å†åˆ†å‰²

```python
def step_a_split_video(self) -> List[Path]:
    # 1. ffmpegã§PCM WAVæŠ½å‡º
    self.extract_mono_wav(input_video, temp_wav, 16000)
    
    # 2. VADã§éŸ³å£°åŒºé–“æ¤œå‡º
    raw_segs = self.detect_speech_segments(temp_wav, 30, vad_mode=2)
    
    # 3. ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµ±åˆãƒ»åˆ†å‰²
    merged = self.merge_segments(raw_segs, max_gap=0.6)
    clips = self.split_long_segments(merged, max_len=180)
    
    # 4. ffmpeg -c copyã§é«˜é€Ÿåˆ†å‰²
    for i, (start, end) in enumerate(clips):
        self.cut_video(input_video, start, end, f"clip_{i:04d}.mp4")
```

**æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆ**:
- å†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãªã—ï¼ˆ`-c copy`ï¼‰ã§é«˜é€Ÿå‡¦ç†
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
- å¤±æ•—ã‚¯ãƒªãƒƒãƒ—ã®å€‹åˆ¥ãƒªãƒˆãƒ©ã‚¤

---

### B. BlendShapeä¿‚æ•°æŠ½å‡º (`step_b_extract_blendshapes`)

**ç›®çš„**: MediaPipe Face Landmarker v2ã§52æ¬¡å…ƒARKit BlendShapeä¿‚æ•°ã‚’æŠ½å‡º

```mermaid
graph LR
    A[å‹•ç”»ã‚¯ãƒªãƒƒãƒ—] --> B[OpenCV<br/>ãƒ•ãƒ¬ãƒ¼ãƒ èª­ã¿è¾¼ã¿]
    B --> C[MediaPipe<br/>Face Landmarker v2]
    C --> D{é¡”æ¤œå‡º<br/>æˆåŠŸ?}
    D -->|YES| E[52æ¬¡å…ƒBlendShape<br/>ä¿‚æ•°æŠ½å‡º]
    D -->|NO| F[ã‚¼ãƒ­ä¿‚æ•°<br/>å“è³ªãƒã‚¹ã‚¯=False]
    E --> G[å“è³ªãƒã‚¹ã‚¯=True]
    F --> H[ä¿‚æ•°ãƒªã‚¹ãƒˆè“„ç©]
    G --> H
    H --> I[CSV/JSONå‡ºåŠ›]
    
    style A fill:#e1f5fe
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#e0f2f1
    style F fill:#ffebee
```

**æŠ€è¡“è©³ç´°**:
- **MediaPipe v2**: Googleæœ€æ–°ã®é¡”èªè­˜ãƒ¢ãƒ‡ãƒ«
- **ARKitäº’æ›**: Appleæ¨™æº–ã®52 BlendShapeä¿‚æ•°
- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†**: GPUåŠ é€Ÿå¯¾å¿œ
- **å“è³ªç›£è¦–**: æ¤œå‡ºå¤±æ•—ãƒ•ãƒ¬ãƒ¼ãƒ ã®è‡ªå‹•ãƒãƒ¼ã‚­ãƒ³ã‚°

```python
def step_b_extract_blendshapes(self, clip_path: Path):
    cap = cv2.VideoCapture(str(clip_path))
    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    
    coeffs_list = []
    quality_mask = []  # True: æ­£å¸¸, False: æ¤œå‡ºå¤±æ•—
    
    for frame_idx, frame in enumerate(video_frames):
        timestamp_ms = int(1000 * frame_idx / fps)
        coeffs = self.landmarker.process_frame(frame, timestamp_ms)
        
        # å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆå…¨ã¦0.0ã¯æ¤œå‡ºå¤±æ•—ï¼‰
        is_valid = not all(c == 0.0 for c in coeffs)
        quality_mask.append(is_valid)
        coeffs_list.append(coeffs)
    
    return coeffs_list, blendshape_names, quality_mask, fps
```

**å‡ºåŠ›ã•ã‚Œã‚‹52 BlendShapeä¿‚æ•°ä¾‹**:
- `jawOpen`, `eyeBlinkLeft/Right`
- `browInnerUp`, `browOuterUpLeft/Right`
- `mouthSmileLeft/Right`, `mouthFrownLeft/Right`
- `cheekPuff`, `noseSneerLeft/Right` ãªã©

---

### C. éŸ³å£°æŠ½å‡º (`step_c_extract_audio`)

**ç›®çš„**: é«˜å“è³ªãª16kHz mono WAVãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ

```python
def step_c_extract_audio(self, clip_path: Path):
    audio_path = self.data_dir / f"{clip_path.stem}_audio.wav"
    
    # ffmpegã§é«˜å“è³ªå¤‰æ›
    self.extract_mono_wav(clip_path, audio_path, 16000)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
    with wave.open(str(audio_path), 'rb') as wf:
        metadata = {
            "sample_rate": wf.getframerate(),
            "channels": wf.getnchannels(),
            "frames": wf.getnframes(),
            "duration": wf.getnframes() / wf.getframerate()
        }
    
    return audio_path, metadata
```

---

### D. éŸ³å£°ç‰¹å¾´é‡æŠ½å‡º (`step_d_extract_audio_features`)

**ç›®çš„**: æ©Ÿæ¢°å­¦ç¿’å‘ã‘åŒ…æ‹¬çš„éŸ³å£°ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã®ç”Ÿæˆ

```mermaid
graph TD
    A[éŸ³å£°WAVãƒ•ã‚¡ã‚¤ãƒ«] --> B[librosa.load<br/>16kHzèª­ã¿è¾¼ã¿]
    B --> C[hop_lengthè¨ˆç®—<br/>æ˜ åƒFPSåŒæœŸ]
    
    C --> D[Melã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ <br/>80æ¬¡å…ƒ]
    C --> E[MFCC<br/>13æ¬¡å…ƒ]
    C --> F[ZCR<br/>ã‚¼ãƒ­ã‚¯ãƒ­ãƒƒã‚·ãƒ³ã‚°ç‡]
    C --> G[ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«é‡å¿ƒ]
    C --> H[RMSã‚¨ãƒãƒ«ã‚®ãƒ¼]
    
    B --> I[pyworld F0æŠ½å‡º<br/>HARVESTç®—æ³•]
    
    D --> J[æ˜ åƒãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã«<br/>ãƒªã‚µã‚¤ã‚ºè£œé–“]
    E --> J
    F --> J
    G --> J
    H --> J
    I --> J
    
    J --> K[NPZåœ§ç¸®ä¿å­˜<br/>synchronized features]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#fce4ec
    style F fill:#f1f8e9
    style G fill:#e0f2f1
    style H fill:#fff8e1
    style I fill:#e8eaf6
    style J fill:#f3e5f5
    style K fill:#e1f5fe
```

**æŠ½å‡ºç‰¹å¾´é‡**:

1. **Melã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ  (80æ¬¡å…ƒ)**
   - äººé–“ã®è´è¦šç‰¹æ€§ã«åŸºã¥ãå‘¨æ³¢æ•°è¡¨ç¾
   - æ·±å±¤å­¦ç¿’ã§ã®éŸ³å£°èªè­˜ã«æœ€é©

2. **F0ï¼ˆåŸºæœ¬å‘¨æ³¢æ•°ï¼‰**
   - pyworld HARVESTã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä½¿ç”¨
   - æ„Ÿæƒ…è¡¨ç¾ã¨é«˜ç›¸é–¢

3. **MFCCï¼ˆ13æ¬¡å…ƒï¼‰**
   - å¾“æ¥ã®éŸ³å£°èªè­˜ã§æ¨™æº–çš„ãªç‰¹å¾´é‡
   - è©±è€…ä¸å¤‰æ€§ãŒé«˜ã„

4. **ã‚¼ãƒ­ã‚¯ãƒ­ãƒƒã‚·ãƒ³ã‚°ç‡ï¼ˆZCRï¼‰**
   - éŸ³å£°ã®æœ‰å£°/ç„¡å£°åˆ¤å®š
   - ãƒã‚¤ã‚ºé™¤å»ã«æœ‰åŠ¹

5. **ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«é‡å¿ƒ**
   - éŸ³è‰²ã®æ˜æš—ã‚’è¡¨ç¾
   - æ„Ÿæƒ…èªè­˜ã«é‡è¦

6. **RMSã‚¨ãƒãƒ«ã‚®ãƒ¼**
   - éŸ³é‡ãƒ¬ãƒ™ãƒ«ã®æ™‚ç³»åˆ—å¤‰åŒ–
   - æ„Ÿæƒ…ã®å¼·åº¦ã¨ç›¸é–¢

```python
def step_d_extract_audio_features(self, audio_path: Path, video_fps: float, num_frames: int):
    # éŸ³å£°èª­ã¿è¾¼ã¿
    y, sr = librosa.load(audio_path, sr=16000)
    
    # æ˜ åƒãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆã«åˆã‚ã›ãŸhop_length
    hop_length = int(sr / video_fps)
    
    # å„ç‰¹å¾´é‡æŠ½å‡º
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80, hop_length=hop_length)
    f0, timeaxis = pw.harvest(y.astype(np.float64), sr, frame_period=1000.0/video_fps)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=hop_length)
    zcr = librosa.feature.zero_crossing_rate(y, hop_length=hop_length)
    
    # æ˜ åƒãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã«åŒæœŸãƒªã‚µã‚¤ã‚º
    features = {
        "mel_spectrogram": resize_feature(mel_spec, num_frames),
        "f0": resize_feature(f0, num_frames),
        "mfcc": resize_feature(mfcc, num_frames),
        "zcr": resize_feature(zcr[0], num_frames),
        # ...
    }
    
    return features
```

**åŒæœŸå‡¦ç†ã®é‡è¦æ€§**:
- éŸ³å£°ç‰¹å¾´é‡ã¨æ˜ åƒãƒ•ãƒ¬ãƒ¼ãƒ ã®å®Œå…¨åŒæœŸ
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨æƒ…ç”Ÿæˆã«å¿…é ˆ
- è£œé–“ã«ã‚ˆã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ æ•°èª¿æ•´

---

### E. å“è³ªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  (`step_e_quality_control`)

**ç›®çš„**: ãƒ‡ãƒ¼ã‚¿å“è³ªã®è‡ªå‹•è©•ä¾¡ãƒ»æ”¹å–„ãƒ»ä¿è¨¼

```mermaid
graph TD
    A[BlendShapeä¿‚æ•°<br/>å“è³ªãƒã‚¹ã‚¯] --> B[å“è³ªçµ±è¨ˆè¨ˆç®—]
    A --> C[E-1. ç•°å¸¸å€¤æ¤œå‡º<br/>IQRæ³•]
    C --> D[è¿‘å‚å€¤ä¸­å¤®å€¤<br/>ã«ã‚ˆã‚‹ä¿®æ­£]
    D --> E[E-2. æ¬ æå€¤è£œé–“]
    E --> F{æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿<br/>4å€‹ä»¥ä¸Š?}
    F -->|YES| G[3æ¬¡ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³è£œé–“<br/>é«˜å“è³ª]
    F -->|NO| H[ç·šå½¢è£œé–“<br/>ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯]
    G --> I[å€¤åŸŸåˆ¶é™<br/>0.0-1.0]
    H --> I
    I --> J[E-3. ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°<br/>EMAé©ç”¨]
    J --> K[æœ€çµ‚å“è³ªçµ±è¨ˆ]
    K --> L[ç·åˆå“è³ªã‚¹ã‚³ã‚¢<br/>0-100ç‚¹]
    
    style A fill:#e1f5fe
    style C fill:#ffebee
    style E fill:#f3e5f5
    style F fill:#fff3e0
    style G fill:#e8f5e8
    style H fill:#fce4ec
    style J fill:#f1f8e9
    style L fill:#e0f2f1
```

#### E-1. ç•°å¸¸å€¤æ¤œå‡ºãƒ»ä¿®æ­£ (`_detect_and_fix_outliers`)

**IQRï¼ˆå››åˆ†ä½ç¯„å›²ï¼‰æ³•ã«ã‚ˆã‚‹ç•°å¸¸å€¤æ¤œå‡º**:
```python
def _detect_and_fix_outliers(self, coeffs_array, quality_array):
    for coeff_idx in range(coeffs_array.shape[1]):
        valid_values = coeffs_array[valid_indices, coeff_idx]
        
        # IQRè¨ˆç®—
        q1, q3 = np.percentile(valid_values, [25, 75])
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        # BlendShapeç‰¹æœ‰ã®åˆ¶é™ï¼ˆ0-1ç¯„å›²ï¼‰
        lower_bound = max(lower_bound, -0.1)
        upper_bound = min(upper_bound, 1.1)
        
        # ç•°å¸¸å€¤ã‚’è¿‘å‚å€¤ã®ä¸­å¤®å€¤ã§ç½®æ›
        for i in valid_indices:
            if value < lower_bound or value > upper_bound:
                neighbors = get_neighboring_values(i, coeffs_array)
                coeffs_array[i, coeff_idx] = np.median(neighbors)
```

#### E-2. æ¬ æå€¤è£œé–“ (`_interpolate_missing_values`)

**ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–è£œé–“ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:
```python
def _interpolate_missing_values(self, coeffs_array, quality_array):
    for coeff_idx in range(coeffs_array.shape[1]):
        valid_indices = np.where(quality_array)[0]
        invalid_indices = np.where(~quality_array)[0]
        
        if len(valid_indices) >= 4:
            # 3æ¬¡ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³è£œé–“ï¼ˆé«˜å“è³ªï¼‰
            f = interpolate.interp1d(valid_indices, valid_values, 
                                   kind='cubic', bounds_error=False)
            coeffs_array[invalid_indices, coeff_idx] = f(invalid_indices)
        else:
            # ç·šå½¢è£œé–“ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            coeffs_array[invalid_indices, coeff_idx] = np.interp(
                invalid_indices, valid_indices, valid_values)
        
        # å€¤åŸŸåˆ¶é™ï¼ˆBlendShapeã¯0-1ï¼‰
        coeffs_array[:, coeff_idx] = np.clip(coeffs_array[:, coeff_idx], 0.0, 1.0)
```

#### E-3. å“è³ªã‚¹ã‚³ã‚¢ç®—å‡º (`_calculate_quality_score`)

**ç·åˆå“è³ªæŒ‡æ¨™ï¼ˆ0-100ç‚¹ï¼‰**:
```python
def _calculate_quality_score(self, coeffs_array, quality_array):
    # 1. æœ‰åŠ¹ãƒ•ãƒ¬ãƒ¼ãƒ ç‡ï¼ˆ40%ï¼‰
    validity_score = quality_array.sum() / len(quality_array) * 100
    
    # 2. å‹•ãã®æ»‘ã‚‰ã‹ã•ï¼ˆ40%ï¼‰- å¤‰åŒ–ç‡ã®åˆ†æ•£ã®é€†æ•°
    smoothness_scores = []
    for coeff_idx in range(coeffs_array.shape[1]):
        diff = np.diff(coeffs_array[:, coeff_idx])
        smoothness = 1.0 / (1.0 + np.std(diff))
        smoothness_scores.append(smoothness)
    smoothness_score = np.mean(smoothness_scores) * 100
    
    # 3. å€¤ã®å¦¥å½“æ€§ï¼ˆ20%ï¼‰- 0-1ç¯„å›²å†…
    validity_ratio = np.mean((coeffs_array >= 0) & (coeffs_array <= 1)) * 100
    
    # é‡ã¿ä»˜ãç·åˆã‚¹ã‚³ã‚¢
    total_score = (validity_score * 0.4 + smoothness_score * 0.4 + validity_ratio * 0.2)
    return min(100.0, max(0.0, total_score))
```

---

### F. ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ»çµ±åˆ (`step_f_save_data`)

**ç›®çš„**: æ©Ÿæ¢°å­¦ç¿’ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªãƒãƒ«ãƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡ºåŠ›

#### ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼:

1. **CSV** - BlendShapeä¿‚æ•° 
   ```csv
   frame,_neutral,browDownLeft,browDownRight,...
   0,0.0035,0.0150,0.0199,...
   1,0.0033,0.0100,0.0248,...
   ```

2. **NPZ** - éŸ³å£°ç‰¹å¾´é‡ï¼ˆåœ§ç¸®æ¸ˆã¿ï¼‰
   ```python
   np.savez_compressed(
       features_npz,
       mel_spectrogram=features["mel_spectrogram"],  # (80, frames)
       f0=features["f0"],                           # (frames,)
       mfcc=features["mfcc"],                       # (13, frames)
       zcr=features["zcr"],                         # (frames,)
       spectral_centroids=features["spectral_centroids"],  # (frames,)
       rms=features["rms"]                          # (frames,)
   )
   ```

3. **NPY** - å“è³ªãƒã‚¹ã‚¯
   ```python
   np.save(mask_npy, np.array(quality_mask))  # Boolean array
   ```

4. **JSON** - çµ±åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
   ```json
   {
     "clip_path": "clips/clip_0000.mp4",
     "frames": 302,
     "missing_frames": 0,
     "quality_report": {
       "quality_score": 99.5,
       "interpolated_frames": 0,
       "outlier_frames": 2
     },
     "audio_metadata": {
       "sample_rate": 16000,
       "duration": 10.036625
     },
     "blendshape_names": ["_neutral", "browDownLeft", ...]
   }
   ```

---

## ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ©Ÿèƒ½

### HTMLè¦–è¦šåŒ–ãƒ¬ãƒãƒ¼ãƒˆ
- å“è³ªçµ±è¨ˆã®ç›´æ„Ÿçš„ãªè¡¨ç¤º
- æ¨å¥¨äº‹é …ã®è‡ªå‹•ç”Ÿæˆ
- ä½¿ç”¨æ–¹æ³•ã‚¬ã‚¤ãƒ‰

### JSONè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
- æ©Ÿæ¢°å¯èª­ãªå®Œå…¨çµ±è¨ˆ
- å“è³ªåˆ†å¸ƒåˆ†æ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™

### æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ ä¾‹
```python
if avg_quality >= 80:
    recommendations.append({
        "type": "success",
        "title": "é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
        "description": f"å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {avg_quality:.1f}",
        "action": "ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯æ©Ÿæ¢°å­¦ç¿’ã«é©ã—ã¦ã„ã¾ã™ã€‚"
    })
```

---

## ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ»å“è³ªä¿è¨¼

```mermaid
graph TD
    A[ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ<br/>test_make_dataset.py] --> B[å˜ä½“ãƒ†ã‚¹ãƒˆ<br/>11é …ç›®]
    
    B --> C[ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–]
    B --> D[VADã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†]
    B --> E[å“è³ªç®¡ç†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ]
    B --> F[ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼]
    B --> G[ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°]
    B --> H[ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹å‡¦ç†]
    
    C --> I[æˆåŠŸ/å¤±æ•—åˆ¤å®š]
    D --> I
    E --> I
    F --> I
    G --> I
    H --> I
    
    I --> J[ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ<br/>æˆåŠŸç‡100%]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
    style I fill:#fff3e0
    style J fill:#e0f2f1
```

### åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ (`test_make_dataset.py`)

**ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸**:
- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
- VADã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†
- å“è³ªç®¡ç†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹å‡¦ç†

**å®Ÿè¡Œæ–¹æ³•**:
```bash
python test_make_dataset.py
```

**ãƒ†ã‚¹ãƒˆçµæœä¾‹**:
```
å®Ÿè¡Œãƒ†ã‚¹ãƒˆæ•°: 11
æˆåŠŸ: 11
å¤±æ•—: 0
ã‚¨ãƒ©ãƒ¼: 0
```

---

## ğŸ¯ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ´»ç”¨æ–¹æ³•

```mermaid
graph LR
    A[ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ] --> B[ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿<br/>JSON]
    B --> C[BlendShapeä¿‚æ•°<br/>CSV/Pandas]
    B --> D[éŸ³å£°ç‰¹å¾´é‡<br/>NPZ/NumPy]
    B --> E[å“è³ªãƒã‚¹ã‚¯<br/>NPY]
    
    C --> F[æ©Ÿæ¢°å­¦ç¿’<br/>ç‰¹å¾´é‡X]
    D --> F
    E --> G[ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°<br/>å“è³ªä¿è¨¼]
    
    F --> H[éŸ³å£°â†’è¡¨æƒ…<br/>äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«]
    G --> H
    
    H --> I[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ <br/>è¡¨æƒ…ç”Ÿæˆ]
    H --> J[VTuber<br/>ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³]
    H --> K[æ„Ÿæƒ…èªè­˜<br/>ã‚·ã‚¹ãƒ†ãƒ ]
    
    style A fill:#e1f5fe
    style F fill:#e8f5e8
    style H fill:#fff3e0
    style I fill:#fce4ec
    style J fill:#f1f8e9
    style K fill:#e0f2f1
```

### Python ã§ã®èª­ã¿è¾¼ã¿ä¾‹
```python
import numpy as np
import pandas as pd
import json

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
with open('data/clip_0000_metadata.json', 'r') as f:
    metadata = json.load(f)

# BlendShapeä¿‚æ•°èª­ã¿è¾¼ã¿
blendshapes = pd.read_csv(metadata['blendshapes_path'])

# éŸ³å£°ç‰¹å¾´é‡èª­ã¿è¾¼ã¿  
audio_features = np.load(metadata['audio_features_path'])
mel_spec = audio_features['mel_spectrogram']  # (80, frames)
f0 = audio_features['f0']                     # (frames,)

# å“è³ªãƒã‚¹ã‚¯èª­ã¿è¾¼ã¿
quality_mask = np.load(metadata['quality_mask_path'])  # (frames,)

# åŒæœŸç¢ºèª
assert len(blendshapes) == mel_spec.shape[1] == len(f0) == len(quality_mask)
```

### æ©Ÿæ¢°å­¦ç¿’ã§ã®åˆ©ç”¨ä¾‹
```python
# éŸ³å£°ç‰¹å¾´é‡ â†’ BlendShapeäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
X = np.concatenate([
    mel_spec.T,  # (frames, 80)
    f0.reshape(-1, 1),  # (frames, 1)
    mfcc.T  # (frames, 13)
], axis=1)  # (frames, 94)

y = blendshapes.iloc[:, 1:].values  # (frames, 52) - exclude frame column

# å“è³ªãƒã‚¹ã‚¯ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
valid_mask = quality_mask
X_clean = X[valid_mask]
y_clean = y[valid_mask]

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100)
model.fit(X_clean, y_clean)
```

---

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

```mermaid
graph TD
    A[ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–] --> B[å‡¦ç†é€Ÿåº¦]
    A --> C[ãƒ¡ãƒ¢ãƒªåŠ¹ç‡]
    A --> D[ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£]
    
    B --> E[VADåˆ†å‰²<br/>30åˆ†â†’90ç§’]
    B --> F[BlendShapeæŠ½å‡º<br/>1000ãƒ•ãƒ¬ãƒ¼ãƒ â†’30ç§’]
    B --> G[éŸ³å£°ç‰¹å¾´é‡<br/>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¯”2å€é€Ÿ]
    B --> H[å“è³ªç®¡ç†<br/>ç·šå½¢æ™‚é–“O(n)]
    
    C --> I[ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†<br/>ãƒ¡ãƒ¢ãƒªæœ€å°åŒ–]
    C --> J[NPZåœ§ç¸®<br/>50%å‰Šæ¸›]
    C --> K[ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«<br/>è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—]
    
    D --> L[ä¸¦åˆ—å‡¦ç†å¯¾å¿œ]
    D --> M[ã‚¯ãƒªãƒƒãƒ—å˜ä½<br/>ç‹¬ç«‹å‡¦ç†]
    D --> N[éƒ¨åˆ†ãƒªãƒˆãƒ©ã‚¤<br/>å¤±æ•—æ™‚å›å¾©]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style D fill:#fce4ec
    style E fill:#f1f8e9
    style F fill:#f1f8e9
    style G fill:#f1f8e9
    style H fill:#f1f8e9
```

### å‡¦ç†é€Ÿåº¦
- **VADåˆ†å‰²**: 30åˆ†å‹•ç”» â†’ 90ç§’
- **BlendShapeæŠ½å‡º**: 1000ãƒ•ãƒ¬ãƒ¼ãƒ  â†’ 30ç§’
- **éŸ³å£°ç‰¹å¾´é‡**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¯”2å€é€Ÿ
- **å“è³ªç®¡ç†**: ç·šå½¢æ™‚é–“è¤‡é›‘åº¦O(n)

### ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€å°åŒ–
- NPZåœ§ç¸®ã§ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡50%å‰Šæ¸›
- ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

### ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£
- ä¸¦åˆ—å‡¦ç†å¯¾å¿œè¨­è¨ˆ
- ã‚¯ãƒªãƒƒãƒ—å˜ä½ã®ç‹¬ç«‹å‡¦ç†
- å¤±æ•—æ™‚ã®éƒ¨åˆ†ãƒªãƒˆãƒ©ã‚¤

---

## ğŸ”§ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãƒ»æ‹¡å¼µ

```mermaid
graph TD
    A[ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãƒ»æ‹¡å¼µ] --> B[ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°]
    A --> C[æ–°æ©Ÿèƒ½è¿½åŠ ]
    
    B --> D[é«˜ç²¾åº¦è¨­å®š<br/>--vad_mode 3<br/>--frame_ms 10]
    B --> E[é«˜é€Ÿè¨­å®š<br/>--vad_mode 1<br/>çŸ­ã‚¯ãƒªãƒƒãƒ—]
    
    C --> F[ã‚«ã‚¹ã‚¿ãƒ éŸ³å£°ç‰¹å¾´é‡<br/>è¿½åŠ å®Ÿè£…]
    C --> G[ä»–ã®é¡”èªè­˜ãƒ¢ãƒ‡ãƒ«<br/>çµ±åˆå¯¾å¿œ]
    C --> H[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ <br/>ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°]
    C --> I[GPUåŠ é€Ÿ<br/>æœ€é©åŒ–]
    
    F --> J[ãƒ—ãƒ©ã‚°ã‚¤ãƒ³<br/>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£]
    G --> J
    H --> K[WebRTC<br/>çµ±åˆ]
    I --> L[CUDA/OpenCL<br/>å¯¾å¿œ]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style D fill:#fce4ec
    style E fill:#fce4ec
    style F fill:#f1f8e9
    style G fill:#f1f8e9
    style H fill:#f1f8e9
    style I fill:#f1f8e9
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
```bash
# é«˜ç²¾åº¦è¨­å®šï¼ˆå‡¦ç†æ™‚é–“å¢—ï¼‰
python make_dataset.py input.mp4 \
  --vad_mode 3 \
  --frame_ms 10 \
  --smooth

# é«˜é€Ÿè¨­å®šï¼ˆç²¾åº¦ä½ä¸‹ï¼‰
python make_dataset.py input.mp4 \
  --vad_mode 1 \
  --min_clip 10 \
  --max_clip 60
```

### æ–°æ©Ÿèƒ½è¿½åŠ ä¾‹
- ã‚«ã‚¹ã‚¿ãƒ éŸ³å£°ç‰¹å¾´é‡ã®è¿½åŠ 
- ä»–ã®é¡”èªè­˜ãƒ¢ãƒ‡ãƒ«ã¨ã®çµ±åˆ
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ
- GPUåŠ é€Ÿã®æœ€é©åŒ–

---

## ğŸ“š æŠ€è¡“å‚è€ƒæ–‡çŒ®

- **MediaPipe**: [Google AI](https://mediapipe.dev/)
- **ARKit BlendShapes**: [Apple Developer](https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation)
- **WebRTC VAD**: [WebRTC Project](https://webrtc.org/)
- **librosa**: [Audio Analysis Library](https://librosa.org/)
- **pyworld**: [WORLD Vocoder](https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder)

---

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt